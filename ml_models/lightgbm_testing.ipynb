{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! pip install smart_open\n",
    "# ! pip install xgboost\n",
    "# ! pip install category_encoders\n",
    "# ! pip install feature-engine\n",
    "# ! pip install lightgbm\n",
    "# ! pip install catboost\n",
    "# ! pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smart_open import smart_open\n",
    "import lightgbm as lgb\n",
    "\n",
    "# trainpath = 's3://sagemaker-shared-resources/model_outputs/fraud_model_6/intermediate_data/X_train_20210603.csv'\n",
    "# testpath = 's3://sagemaker-shared-resources/model_outputs/fraud_model_6/intermediate_data/X_test_20210603.csv'\n",
    "# X_train = pd.read_csv(smart_open(trainpath), low_memory = False)\n",
    "# X_test = pd.read_csv(smart_open(testpath), low_memory = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformed_trainpath = 's3://sagemaker-shared-resources/model_outputs/fraud_model_6/intermediate_data/X_train_transformed_06242021.csv'\n",
    "transformed_testpath = 's3://sagemaker-shared-resources/model_outputs/fraud_model_6/intermediate_data/X_test_transformed_06242021.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(smart_open(transformed_trainpath), low_memory = False)\n",
    "X_test = pd.read_csv(smart_open(transformed_testpath), low_memory = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = X_train['dep_var']\n",
    "X_train = X_train.drop('dep_var', axis = 1)\n",
    "\n",
    "#HOLD-OUT SET BELOW\n",
    "X_test = pd.read_csv(smart_open(transformed_testpath), low_memory = False)\n",
    "y_test = X_test['dep_var']\n",
    "X_test = X_test.drop('dep_var', axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CANNOT TRAIN THESE FEATURES ON NEURAL NETWORK\n",
    "\n",
    "# for c in X_train.columns:\n",
    "#     if max(X_train[c]) > 2:\n",
    "#         print (c, max(X_train[c]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "val_select = np.random.choice(len(y_train), len(y_train) // 5)\n",
    "X_val = X_train.iloc[val_select]\n",
    "y_val = y_train[val_select]\n",
    "X_val.shape, y_val.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = X_train.drop(val_select).reset_index(drop=True)\n",
    "y_train = y_train.drop(val_select).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "from io import StringIO\n",
    "import os\n",
    "import shutil"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('fraud_models/fraud-model-6.0.0'))\n",
    "sys.path.append(module_path+\"/sagemaker_scripts\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# basic modules\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "#parsing modules\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "from datetime import datetime, timezone"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#custom parsing transfomers\n",
    "from parsing_pipeline import FeatureSelector, ParsingTransformer, MissingHandler, FeatureDropper, DateMissingTransformer\n",
    "from data_integrity_pipeline import DataIntegrityTransformer\n",
    "from feature_engineering import DateTransformer, idaParser, MissingIndicatorTransformer\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Pipeline imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import _VectorizerMixin\n",
    "from sklearn.feature_selection._base import SelectorMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#evaluation imports\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, f1_score,recall_score,precision_score, average_precision_score\n",
    "from sklearn.model_selection import cross_validate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#ML Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n",
    "from feature_engine.encoding import RareLabelEncoder, MeanEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "\n",
    "#import featuretools as ft"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataframe_cols = list(X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_features = list(set([*numeric_vars, *all_date_vars, *categorical_vars]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsing_pipeline = Pipeline(steps = [('cleaner', ParsingTransformer(df_names = dataframe_cols,\n",
    "                                                                    tmx_unclean_vars = tmx_vars,\n",
    "                                                                    exact_match_vars = find_replace_exact_vars,\n",
    "                                                                    regex_match_vars = find_replace_regex_vars)),\n",
    "                                     ('date_impute', DateMissingTransformer(datemissing_cols = date_missing_vars,\n",
    "                                                                            reference_cols = date_missing_ref_vars,\n",
    "                                                                           reference_dict = date_impute_dict)),\n",
    "                                ('feature_selector', FeatureSelector(all_features)),\n",
    "                                ('data_integrity', DataIntegrityTransformer(all_date_vars,\n",
    "                                                                           numeric_vars,\n",
    "                                                                           categorical_vars))\n",
    "                                        ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#Pipeline imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import _VectorizerMixin\n",
    "from sklearn.feature_selection._base import SelectorMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "\n",
    "\n",
    "#evaluation imports\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, f1_score,recall_score,precision_score, average_precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "# inference functions ---------------\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    model_fn re-defined to accomodate Sagemaker Python SDK.\n",
    "\n",
    "    Args:\n",
    "        model_dir: Default location where model is stored on the instance\n",
    "\n",
    "    Returns: a pipeline object which can be used to predict with\n",
    "\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    A modified predict_fn for Scikit-learn. Calls a model on data deserialized in input_fn. Returns probabilities instead of predictions.\n",
    "\n",
    "    Args:\n",
    "        input_data: input data (Numpy array) for prediction deserialized by input_fn\n",
    "        model: Scikit-learn model loaded in memory by model_fn\n",
    "\n",
    "    Returns: probability of fraud\n",
    "    \"\"\"\n",
    "    pred_prob = model.predict_proba(input_data)\n",
    "    return pred_prob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fe_pipeline = Pipeline(steps = [(\"ida_fe\", idaParser(ida_impute_vars)),\n",
    "#                                 ('dates_fe', DateTransformer(fe_date_vars,\n",
    "#                                                              fe_timestamp_vars,\n",
    "#                                                              all_date_vars)),\n",
    "#                                 ('missing_dealer', MissingHandler())\n",
    "#                                 ])\n",
    "\n",
    "# #categorical and numeric transformers\n",
    "# categorical_transformer = Pipeline(steps = [('cat_imputer', CategoricalImputer(imputation_method = 'missing', fill_value='Missing')),\n",
    "#                                       ('rare_label_encoder',RareLabelEncoder(tol=0.02, n_categories=3,replace_with='Other')),\n",
    "#                                            ('cat_encoder', ce.TargetEncoder()),\n",
    "#                                            ('scaler', StandardScaler())\n",
    "#                                            ])\n",
    "\n",
    "# numeric_transformer = Pipeline(steps = [('num_imputer', SimpleImputer()),\n",
    "#                                        ('scaler', StandardScaler())\n",
    "#                                        ])\n",
    "\n",
    "# #combining final cat and num transformer pipelines\n",
    "# preprocessor = ColumnTransformer(transformers=[\n",
    "#         (\"num_t\", numeric_transformer, make_column_selector(dtype_exclude=\"object\")),\n",
    "#         (\"cat_t\", categorical_transformer, make_column_selector(dtype_include=\"object\"))],\n",
    "#                                 n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# clf = Pipeline(steps=[('parser', parsing_pipeline),\n",
    "#                   ('feature creation', fe_pipeline),\n",
    "#                   ('preprocessor', preprocessor),\n",
    "#                   ('classifier', xgb.XGBClassifier())])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_missing_imputer = SimpleImputer()\n",
    "percent_missing = 0.85\n",
    "cat_missing_imputer = CategoricalImputer(imputation_method = 'missing', fill_value='Missing')\n",
    "cat_encoder = ce.TargetEncoder()\n",
    "num_scaler = RobustScaler()\n",
    "tol_rare_label = 0.0017"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# JUNE 22 version\n",
    "\n",
    "#Features that will be analysed and used in initial model training\n",
    "all_features = list(set([*numeric_vars, *all_date_vars, *categorical_vars]))\n",
    "#cleaning data and ensuring formats of variables\n",
    "parsing_pipeline = Pipeline(steps = [('cleaner', ParsingTransformer(df_names = dataframe_cols,\n",
    "                                                                tmx_unclean_vars = tmx_vars,\n",
    "                                                                exact_match_vars = find_replace_exact_vars,\n",
    "                                                                regex_match_vars = find_replace_regex_vars)),\n",
    "                                 ('date_impute', DateMissingTransformer(datemissing_cols = date_missing_vars,\n",
    "                                                                        reference_cols = date_missing_ref_vars,\n",
    "                                                                       reference_dict = date_impute_dict)),\n",
    "                            ('feature_selector', FeatureSelector(all_features)),\n",
    "                            ('data_integrity', DataIntegrityTransformer(all_date_vars,\n",
    "                                                                       numeric_vars,\n",
    "                                                                       categorical_vars))\n",
    "                                    ])\n",
    "#creating features\n",
    "fe_pipeline = Pipeline(steps = [(\"ida_fe\", idaParser(ida_impute_vars)),\n",
    "                                ('dates_fe', DateTransformer(fe_date_vars,\n",
    "                                                             fe_timestamp_vars,\n",
    "                                                             all_date_vars)),\n",
    "                                ('missing_dealer', MissingHandler(percent_missing))\n",
    "                                ])\n",
    "#categorical and numeric transformers\n",
    "categorical_transformer = Pipeline(steps = [('cat_imputer', cat_missing_imputer),\n",
    "                                      ('rare_label_encoder', RareLabelEncoder(tol=tol_rare_label, n_categories=3,replace_with='Other')),\n",
    "                                           ('cat_encoder', cat_encoder)\n",
    "                                           ])\n",
    "numeric_transformer = Pipeline(steps = [('num_imputer', num_missing_imputer),\n",
    "                                       ('scaler', num_scaler)\n",
    "                                       ])\n",
    "#combining final cat and num transformer pipelines\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        (\"num_t\", numeric_transformer, make_column_selector(dtype_exclude=\"object\")),\n",
    "        (\"cat_t\", categorical_transformer, make_column_selector(dtype_include=\"object\"))])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cv_results = cross_validate(clf, X_train, y_train, cv=10,\n",
    "#                                scoring=('average_precision', 'roc_auc'),\n",
    "#                                n_jobs = -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%writefile cv_results_june17.txt\n",
    "\n",
    "# {'fit_time': array([3092.79319549, 2225.58159685, 3121.47579575, 2224.81072187,\n",
    "#         2054.69752979, 2321.39297771, 2171.7442019 , 2588.71676588,\n",
    "#         3289.96349502, 2741.13525772]),\n",
    "#  'score_time': array([229.11062717, 278.21359372, 221.84894419, 284.22967839,\n",
    "#         437.0422523 , 281.2246213 , 350.1626215 , 224.6388104 ,\n",
    "#         173.50640059, 261.87610626]),\n",
    "#  'test_average_precision': array([0.28749455, 0.61527515, 0.78443781, 0.81882475, 0.89038318,\n",
    "#         0.88343627, 0.90505814, 0.78419411, 0.72189914, 0.41737601]),\n",
    "#  'test_roc_auc': array([0.87441938, 0.90931607, 0.97205096, 0.96659792, 0.97365605,\n",
    "#         0.98095623, 0.98167272, 0.96564525, 0.94869597, 0.86197157])}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_transformer = Pipeline(steps=[('parser', parsing_pipeline),\n",
    "                      ('feature creation', fe_pipeline),\n",
    "                      ('preprocessor', preprocessor)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "start = datetime.now().time()\n",
    "# parsing_pipeline.fit_transform(X_train)\n",
    "end =  datetime.now().time()\n",
    "datetime.combine(datetime.today(), end) - datetime.combine(datetime.today(), start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_transformed = data_transformer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_val_transformed = data_transformer.transform(X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_feature_out(estimator, feature_in):\n",
    "    if hasattr(estimator,'get_feature_names'):\n",
    "        if isinstance(estimator, _VectorizerMixin):\n",
    "            # handling all vectorizers\n",
    "            return [f'vec_{f}' \\\n",
    "                for f in estimator.get_feature_names()]\n",
    "        else:\n",
    "            return estimator.get_feature_names()\n",
    "    elif isinstance(estimator, SelectorMixin):\n",
    "        return np.array(feature_in)[estimator.get_support()]\n",
    "    else:\n",
    "        return feature_in\n",
    "\n",
    "def get_ct_feature_names(ct):\n",
    "    # handles all estimators, pipelines inside ColumnTransfomer\n",
    "    # doesn't work when remainder =='passthrough'\n",
    "    # which requires the input column names.\n",
    "    output_features = []\n",
    "    for name, estimator, features in ct.transformers_:\n",
    "        if name!='remainder':\n",
    "            if isinstance(estimator, Pipeline):\n",
    "                current_features = features\n",
    "                for step in estimator:\n",
    "                    current_features = get_feature_out(step, current_features)\n",
    "                features_out = current_features\n",
    "            else:\n",
    "                features_out = get_feature_out(estimator, features)\n",
    "            output_features.extend(features_out)\n",
    "        elif estimator=='passthrough':\n",
    "            output_features.extend(ct._feature_names_in[features])\n",
    "    return output_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "column_names = get_ct_feature_names(data_transformer.named_steps['preprocessor'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_val_transformed_named = pd.DataFrame(X_val_transformed, columns = column_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_transformed_named = pd.DataFrame(transformed_X_train, columns = column_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X_val_transformed = data_transformer.transform(X_val)\n",
    "X_test_transformed_named = pd.DataFrame(X_test_transformed, columns = column_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_transformed_named.to_csv('X_train_transformed_named.csv', index=False)\n",
    "X_test_transformed_named.to_csv('X_test_transformed_named.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle              # import module first\n",
    "f = open('X_val_transformed.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n",
    "pickle.dump(X_val_transformed, f)          # dump data to f\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_transformed_named.to_csv('X_test_transformed_named.csv', index=False)\n",
    "\n",
    "import pickle              # import module first\n",
    "f = open('X_test_transformed.pkl', 'w')   # Pickle file is newly created where foo1.py is\n",
    "pickle.dump(X_test_transformed, f)          # dump data to f\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_transformed_named = pd.read_csv('X_train_transformed_named.csv')\n",
    "\n",
    "f = open('X_val_transformed.pkl', 'r')   # 'r' for reading; can be omitted\n",
    "X_val_transformed = pickle.load(f)         # load file content as mydict\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "lgb_params = {'colsample_bytree': 0.4592204345120233,\n",
    "    'learning_rate': 0.7734962535799013,\n",
    "    'max_depth': 14,\n",
    "    'min_child_weight': 47,\n",
    "    'min_split_gain': 0.47802390488529,\n",
    "    'num_leaves': 39,\n",
    "    'reg_alpha': 0.6038325706218322,\n",
    "    'reg_lambda': 0.26914645113480473,\n",
    "    'subsample': 0.8926876963076777}\n",
    "\n",
    "clflgb = lgb.sklearn.LGBMClassifier(**lgb_params)\n",
    "\n",
    "clflgb.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_val, clflgb.predict(X_val))\n",
    "\n",
    "probas_ = clflgb.predict_proba(X_val)\n",
    "plots_ytrue  = y_val.copy()\n",
    "plots_yscore = probas_[:,1]\n",
    "auc_roc = roc_auc_score(y_true = plots_ytrue, y_score = plots_yscore)\n",
    "auc_pr = average_precision_score(y_true = plots_ytrue, y_score = plots_yscore)\n",
    "print(\"AUC-ROC score is {}\".format(round(auc_roc,4)))\n",
    "print(\"AUC-PR score is {}\".format(round(auc_pr,4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_params = {'colsample_bytree': 0.4592204345120233,\n",
    "    'learning_rate': 0.7734962535799013,\n",
    "    'max_depth': 14,\n",
    "    'min_child_weight': 47,\n",
    "    'min_split_gain': 0.47802390488529,\n",
    "    'num_leaves': 39,\n",
    "    'reg_alpha': 0.6038325706218322,\n",
    "    'reg_lambda': 0.26914645113480473,\n",
    "    'subsample': 0.8926876963076777}\n",
    "\n",
    "clflgb = lgb.sklearn.LGBMClassifier(**lgb_params)\n",
    "\n",
    "clflgb.fit(X_train, y_train)\n",
    "\n",
    "# accuracy_score(y_val, clflgb.predict(X_val))\n",
    "\n",
    "probas_ = clflgb.predict_proba(X_val)\n",
    "plots_ytrue  = y_val.copy()\n",
    "plots_yscore = probas_[:,1]\n",
    "auc_roc = roc_auc_score(y_true = plots_ytrue, y_score = plots_yscore)\n",
    "auc_pr = average_precision_score(y_true = plots_ytrue, y_score = plots_yscore)\n",
    "print(\"AUC-ROC score is {}\".format(round(auc_roc,4)))\n",
    "print(\"AUC-PR score is {}\".format(round(auc_pr,4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train_transformed_named, label=y_train)\n",
    "train_data.get_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_data = lgb.Dataset(X_val_transformed, label=y_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'bagging_fraction': 0.77,\n",
    "        'bagging_freq': 2,\n",
    "        'lambda_l1': 0.7,\n",
    "        'lambda_l2': 2,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 10,\n",
    "        'min_data_in_leaf': 22,\n",
    "        'min_gain_to_split': 0.07,\n",
    "        'min_sum_hessian_in_leaf': 19,\n",
    "        'num_leaves': 20,\n",
    "        'feature_fraction': 1,\n",
    "        'save_binary': True,\n",
    "        'seed': 42,\n",
    "        'feature_fraction_seed': 42,\n",
    "        'bagging_seed': 42,\n",
    "        'drop_seed': 42,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': 'false',\n",
    "        'num_threads': 6\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train_transformed_named, label=y_train)\n",
    "# clf = lgb.train(lgb_params, train_data, 10000, valid_sets=[val_data], verbose_eval=-1, early_stopping_rounds=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clflgb = lgb.sklearn.LGBMClassifier(min_data_in_leaf = 1, min_data_in_bin = 1)\n",
    "# clflgb.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = cross_val_score(clflgb, X_train, y_train, cv=20,scoring='accuracy',n_jobs=-1)\n",
    "score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf.best_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smart_open import smart_open\n",
    "import lightgbm as lgb\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import sklearn.metrics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lgb_params = {'colsample_bytree': 0.4705939,\n",
    "  'learning_rate': 0.01407,\n",
    "  'max_depth': 13,\n",
    "  'min_child_weight': 21,\n",
    "  'min_split_gain': 0.4238,\n",
    "  'n_estimators': 787,\n",
    "  'num_leaves': 182,\n",
    "  'reg_alpha': 0.3619,\n",
    "  'reg_lambda': 0.32818,\n",
    "  'subsample': 0.8505}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lgb_reg_params = {\n",
    "    'max_depth':        hp.choice('max_depth', np.arange(7, 15, 1, dtype=int)),\n",
    "    'learning_rate':    hp.uniform('learning_rate', 0.001, 0.01),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(12, 30, 2, dtype=int)),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 0.6),\n",
    "    'subsample':        hp.uniform('subsample', 0.6, 1),\n",
    "    'num_leaves':       hp.choice('num_leaves', np.arange(5, 200, 1, dtype=int)),\n",
    "    'min_split_gain':   hp.uniform('min_split_gain', 0.3, 0.6),\n",
    "    'reg_alpha':        hp.uniform('reg_alpha', 0.2, 0.5),\n",
    "    'reg_lambda':       hp.uniform('reg_lambda',0.2, 0.7),\n",
    "    'n_estimators':     hp.choice('n_estimators', np.arange(500, 1200, 25, dtype=int)),\n",
    "    'max_delta_step': hp.uniform('max_delta_step',5, 200),\n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    print (params)\n",
    "    lgbm = lgb.sklearn.LGBMClassifier(n_jobs=-1, early_stopping_rounds=None,**params)\n",
    "    score = cross_val_score(lgbm, X_train, y_train, cv=10,scoring='average_precision',n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "trials = Trials()\n",
    "result = fmin(\n",
    "    fn=f,                           # objective function\n",
    "    space=lgb_reg_params,   # parameter space\n",
    "    algo=tpe.suggest,             # surrogate algorithm\n",
    "    max_evals=50,\n",
    "    early_stop_fn=no_progress_loss(iteration_stop_count=10, percent_increase=0.0),\n",
    "    trials=trials# no. of evaluations\n",
    ")\n",
    "print(result)\n",
    "\n",
    "space_eval(lgb_reg_params, result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len([t for t in trials.trials])\n",
    "# amphetamine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = []\n",
    "for t in trials.trials:\n",
    "    try:\n",
    "        print(t['result']['loss'])\n",
    "        loss.append(t['result']['loss'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "np.argmax(loss), np.max(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_values_dict = [t['misc']['vals'] for t in trials.trials][np.argmax(loss)]\n",
    "dict(zip(list(best_values_dict.keys()), [x[0] for x in best_values_dict.values()]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_values_dict = [t['misc']['vals'] for t in trials.trials][np.argmax([t['result']['loss'] for t in trials.trials])]\n",
    "dict(zip(list(best_values_dict.keys()), [x[0] for x in best_values_dict.values()]))\n",
    "# create text file for storing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformed_trainpath = 's3://sagemaker-shared-resources/model_outputs/fraud_model_6/intermediate_data/X_train_transformed_06242021.csv'\n",
    "transformed_testpath = 's3://sagemaker-shared-resources/model_outputs/fraud_model_6/intermediate_data/X_test_transformed_06242021.csv'\n",
    "\n",
    "filtered_trainpath = 's3://sagemaker-shared-resources/model_outputs/fraud_model_6/intermediate_data/X_train_filtered_07072021.csv'\n",
    "filtered_testpath = 's3://sagemaker-shared-resources/model_outputs/fraud_model_6/intermediate_data/X_test_filtered_07072021.csv'\n",
    "\n",
    "X_train = pd.read_csv(smart_open(filtered_trainpath), low_memory = False)\n",
    "X_test = pd.read_csv(smart_open(filtered_testpath), low_memory = False)\n",
    "y_train = X_train['dep_var']\n",
    "X_train = X_train.drop('dep_var', axis = 1)\n",
    "y_test = X_test['dep_var']\n",
    "X_test = X_test.drop('dep_var', axis = 1)\n",
    "\n",
    "np.random.seed(42)\n",
    "val_select = np.random.choice(len(y_train), len(y_train) // 5)\n",
    "X_val = X_train.iloc[val_select]\n",
    "y_val = y_train[val_select]\n",
    "\n",
    "X_train = X_train.drop(val_select).reset_index(drop=True)\n",
    "y_train = y_train.drop(val_select).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lgb_params = {'colsample_bytree': 0.4790533301682829,\n",
    " 'learning_rate': 0.0774950067234158,\n",
    " 'max_depth': 15,\n",
    " 'min_child_weight': 28,\n",
    " 'min_split_gain': 0.3014256129493069,\n",
    " 'n_estimators': 725,\n",
    " 'num_leaves': 98,\n",
    " 'max_delta_step': 8,\n",
    " 'reg_alpha': 0.33684212620847115,\n",
    " 'reg_lambda': 0.6818330325814729,\n",
    " 'subsample': 0.8728280598689245}\n",
    "\n",
    "clflgb = lgb.sklearn.LGBMClassifier(**lgb_params)\n",
    "\n",
    "clflgb.fit(X_train, y_train)\n",
    "\n",
    "# accuracy_score(y_val, clflgb.predict(X_val))\n",
    "\n",
    "probas_ = clflgb.predict_proba(X_val)\n",
    "plots_ytrue  = y_val.copy()\n",
    "plots_yscore = probas_[:,1]\n",
    "auc_roc = roc_auc_score(y_true = plots_ytrue, y_score = plots_yscore)\n",
    "auc_pr = average_precision_score(y_true = plots_ytrue, y_score = plots_yscore)\n",
    "print(\"AUC-ROC score is {}\".format(round(auc_roc,4)))\n",
    "print(\"AUC-PR score is {}\".format(round(auc_pr,4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# zKMODEL\n",
    "\n",
    "# lgb_params = {'colsample_bytree': 0.4705939,\n",
    "#   'learning_rate': 0.001407,\n",
    "#   'max_depth': 13,\n",
    "#   'min_child_weight': 21,\n",
    "#   'min_split_gain': 0.4238,\n",
    "#   'n_estimators': 787,\n",
    "#   'num_leaves': 182,\n",
    "#   'max_delta_step': 2,\n",
    "#   'reg_alpha': 0.3619,\n",
    "#   'reg_lambda': 0.32818,\n",
    "#   'subsample': 0.8505}\n",
    "\n",
    "lgb_params = {'colsample_bytree': 0.5617655338175288,\n",
    "              'learning_rate': 0.00405414048712561,\n",
    "              'max_delta_step': 173.2194970737864,\n",
    "              'max_depth': 9, 'min_child_weight': 14,\n",
    "              'min_split_gain': 0.4017601799355375,\n",
    "              'n_estimators': 775, 'num_leaves': 154,\n",
    "              'reg_alpha': 0.4603849990916753,\n",
    "              'reg_lambda': 0.45386557520650367,\n",
    "              'subsample': 0.8260702841662341}\n",
    "\n",
    "clflgb = lgb.sklearn.LGBMClassifier(**lgb_params)\n",
    "\n",
    "clflgb.fit(X_train, y_train)\n",
    "\n",
    "# accuracy_score(y_test, clflgb.predict(X_test_transformed))\n",
    "\n",
    "probas_ = clflgb.predict_proba(X_val)\n",
    "plots_ytrue  = y_val.copy()\n",
    "plots_yscore = probas_[:,1]\n",
    "auc_roc = roc_auc_score(y_true = plots_ytrue, y_score = plots_yscore)\n",
    "auc_pr = average_precision_score(y_true = plots_ytrue, y_score = plots_yscore)\n",
    "print(\"AUC-ROC score is {}\".format(round(auc_roc,4)))\n",
    "print(\"AUC-PR score is {}\".format(round(auc_pr,4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_values = shap.TreeExplainer(clflgb).shap_values(X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lgb_params = {'colsample_bytree': 0.4705939,\n",
    "  'learning_rate': 0.01407,\n",
    "  'max_depth': 13,\n",
    "  'min_child_weight': 21,\n",
    "  'min_split_gain': 0.4238,\n",
    "  'n_estimators': 787,\n",
    "  'num_leaves': 182,\n",
    "  'reg_alpha': 0.3619,\n",
    "  'reg_lambda': 0.32818,\n",
    "  'subsample': 0.8505}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lgb_params = {'colsample_bytree': 0.5299950340768536,\n",
    "#  'learning_rate': 0.005278780380213163,\n",
    "#  'max_depth': 2,\n",
    "#  'min_child_weight': 3,\n",
    "#  'min_split_gain': 0.3141438019439081,\n",
    "#  'n_estimators': 6,\n",
    "#  'num_leaves': 107,\n",
    "#  'reg_alpha': 0.4762560219705076,\n",
    "#  'reg_lambda': 0.23912490293874628,\n",
    "#  'subsample': 0.9554349613610617}\n",
    "\n",
    "lgb_params = {'colsample_bytree': 0.5617655338175288,\n",
    "              'learning_rate': 0.00405414048712561,\n",
    "              'max_delta_step': 173.2194970737864,\n",
    "              'max_depth': 9, 'min_child_weight': 14,\n",
    "              'min_split_gain': 0.4017601799355375,\n",
    "              'n_estimators': 775, 'num_leaves': 154,\n",
    "              'reg_alpha': 0.4603849990916753,\n",
    "              'reg_lambda': 0.45386557520650367,\n",
    "              'subsample': 0.8260702841662341}\n",
    "\n",
    "lgbm = lgb.sklearn.LGBMClassifier(n_jobs=-1, early_stopping_rounds=None,**lgb_params)\n",
    "\n",
    "cv_results = cross_validate(lgbm, X_train, y_train, cv=10,\n",
    "                               scoring=('average_precision', 'roc_auc'),\n",
    "                               n_jobs = -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('printing final results')\n",
    "auc_pr_mean = cv_results['test_average_precision'].mean()\n",
    "print('AUC-PR = {}'.format(auc_pr_mean))\n",
    "auc_roc_mean = cv_results['test_roc_auc'].mean()\n",
    "print('AUC-ROC = {}'.format(auc_roc_mean))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lgb_params = {'colsample_bytree': 0.4705939,\n",
    "  'learning_rate': 0.01407,\n",
    "  'max_depth': 13,\n",
    "  'min_child_weight': 21,\n",
    "  'min_split_gain': 0.4238,\n",
    "  'n_estimators': 787,\n",
    "  'num_leaves': 182,\n",
    "  'reg_alpha': 0.3619,\n",
    "  'reg_lambda': 0.32818,\n",
    "  'subsample': 0.8505}\n",
    "\n",
    "lgbm = lgb.sklearn.LGBMClassifier(n_jobs=-1, early_stopping_rounds=None,**lgb_params)\n",
    "\n",
    "cv_results = cross_validate(lgbm, X_train_, y_train_, cv=10,\n",
    "                               scoring=('average_precision', 'roc_auc'),\n",
    "                               n_jobs = -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('printing final results')\n",
    "auc_pr_mean = cv_results['test_average_precision'].mean()\n",
    "print('AUC-PR = {}'.format(auc_pr_mean))\n",
    "auc_roc_mean = cv_results['test_roc_auc'].mean()\n",
    "print('AUC-ROC = {}'.format(auc_roc_mean))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_lgbm_varimp(model, train_columns, max_vars=50):\n",
    "\n",
    "    if \"basic.Booster\" in str(model.__class__):\n",
    "        # lightgbm.basic.Booster was trained directly, so using feature_importance() function\n",
    "        cv_varimp_df = pd.DataFrame([train_columns, model.feature_importance()]).T\n",
    "    else:\n",
    "        # Scikit-learn API LGBMClassifier or LGBMRegressor was fitted,\n",
    "        # so using feature_importances_ property\n",
    "        cv_varimp_df = pd.DataFrame([train_columns, model.feature_importances_]).T\n",
    "\n",
    "    cv_varimp_df.columns = ['feature_name', 'varimp']\n",
    "\n",
    "    cv_varimp_df.sort_values(by='varimp', ascending=False, inplace=True)\n",
    "\n",
    "    cv_varimp_df = cv_varimp_df.iloc[0:max_vars]\n",
    "\n",
    "    return cv_varimp_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_lgbm_varimp(clflgb, X_train.columns, max_vars=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
